__author__ = "DDSRem <https://ddsrem.com>"
__all__ = [
    "ShareP115Client",
    "iter_share_files_with_path",
    "get_pid_by_path",
    "get_pickcode_by_path",
    "P115DiskHelper",
]


from dataclasses import dataclass
from datetime import datetime, timezone
from itertools import cycle
from os import PathLike
from pathlib import Path
from time import perf_counter, sleep
from typing import (
    Iterator,
    Literal,
    List,
    Tuple,
    Dict,
    Any,
    Set,
    Optional,
    Callable,
    Coroutine,
    TYPE_CHECKING,
)
from concurrent.futures import ThreadPoolExecutor, Future, as_completed

from cryptography.hazmat.primitives import hashes
from oss2 import StsAuth, Bucket, determine_part_size
from oss2.exceptions import ServerError
from oss2.models import PartInfo
from oss2.utils import b64encode_as_string, SizedFileAdapter
from p115client import P115Client, check_response
from p115client.util import complete_url, posix_escape_name
from p115client.tool.attr import normalize_attr, get_id

from app.core.config import global_vars
from app.log import logger
from app.modules.filemanager.storages import transfer_process
from app.schemas import FileItem, NotificationType
from app.utils.string import StringUtils

from ..core.cache import idpathcacher
from ..core.config import configer
from ..core.message import post_message
from ..db_manager.oper import FileDbHelper
from ..utils.limiter import ApiEndpointCooldown
from ..utils.oopserver import OOPServerRequest


class ShareP115Client(P115Client):
    """
    åˆ†äº«åŒæ­¥ä¸“ç”¨ Client
    """

    def share_snap_cookie(
        self,
        payload: dict,
        /,
        base_url: str | Callable[[], str] = "https://webapi.115.com",
        *,
        async_: Literal[False, True] = False,
        **request_kwargs,
    ) -> dict | Coroutine[Any, Any, dict]:
        """
        è·å–åˆ†äº«é“¾æ¥çš„æŸä¸ªç›®å½•ä¸­çš„æ–‡ä»¶å’Œå­ç›®å½•çš„åˆ—è¡¨ï¼ˆåŒ…å«è¯¦ç»†ä¿¡æ¯ï¼‰

        GET https://webapi.115.com/share/snap

        :payload:
            - share_code: str
            - receive_code: str
            - cid: int | str = 0
            - limit: int = 32
            - offset: int = 0
            - asc: 0 | 1 = <default> ğŸ’¡ æ˜¯å¦å‡åºæ’åˆ—
            - o: str = <default> ğŸ’¡ ç”¨æŸå­—æ®µæ’åº

                - "file_name": æ–‡ä»¶å
                - "file_size": æ–‡ä»¶å¤§å°
                - "user_ptime": åˆ›å»ºæ—¶é—´/ä¿®æ”¹æ—¶é—´
        """
        api = complete_url("/share/snap", base_url=base_url)
        payload = {"cid": 0, "limit": 32, "offset": 0, **payload}
        return self.request(url=api, params=payload, async_=async_, **request_kwargs)


@dataclass
class ApiEndpointInfo:
    """
    API ç«¯ç‚¹ä¿¡æ¯
    """

    endpoint: ApiEndpointCooldown
    api_name: str
    base_url: Optional[str] = None


def iter_share_files_with_path(
    client: str | PathLike | ShareP115Client,
    share_code: str,
    receive_code: str = "",
    cid: int = 0,
    order: Literal[
        "file_name", "file_size", "file_type", "user_utime", "user_ptime", "user_otime"
    ] = "user_ptime",
    asc: Literal[0, 1] = 1,
    max_workers: int = 25,
    speed_mode: Literal[0, 1, 2, 3] = 3,
    **request_kwargs,
) -> Iterator[dict]:
    """
    æ‰¹é‡è·å–åˆ†äº«é“¾æ¥ä¸‹çš„æ–‡ä»¶åˆ—è¡¨

    :param client: 115 å®¢æˆ·ç«¯æˆ– cookies
    :param share_code: åˆ†äº«ç æˆ–é“¾æ¥
    :param receive_code: æ¥æ”¶ç 
    :param cid: ç›®å½•çš„ id
    :param order: æ’åº

        - "file_name": æ–‡ä»¶å
        - "file_size": æ–‡ä»¶å¤§å°
        - "file_type": æ–‡ä»¶ç§ç±»
        - "user_utime": ä¿®æ”¹æ—¶é—´
        - "user_ptime": åˆ›å»ºæ—¶é—´
        - "user_otime": ä¸Šä¸€æ¬¡æ‰“å¼€æ—¶é—´

    :param asc: å‡åºæ’åˆ—ã€‚0: å¦ï¼Œ1: æ˜¯
    :param max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
    :param speed_mode: è¿è¡Œé€Ÿåº¦æ¨¡å¼
        0: æœ€å¿« (0.25s, 0.25s, 0.75s)
        1: å¿« (0.5s, 0.5s, 1.5s)
        2: æ…¢ (1s, 1s, 2s)
        3: æœ€æ…¢ (1.5s, 1.5s, 2s)

    :return: è¿­ä»£å™¨ï¼Œè¿”å›æ­¤åˆ†äº«é“¾æ¥ä¸‹çš„ï¼ˆæ‰€æœ‰æ–‡ä»¶ï¼‰æ–‡ä»¶ä¿¡æ¯
    """
    if isinstance(client, (str, PathLike)):
        client = ShareP115Client(client, check_for_relogin=True)
    speed_configs = {
        0: (0.25, 0.25, 0.75),
        1: (0.5, 0.5, 1.5),
        2: (1.0, 1.0, 2.0),
        3: (1.5, 1.5, 2.0),
    }
    app_http_cooldown, app_https_cooldown, api_cooldown = speed_configs.get(
        speed_mode, speed_configs[1]
    )
    snap_app_http_info = ApiEndpointInfo(
        endpoint=ApiEndpointCooldown(
            api_callable=lambda p: client.share_snap_app(
                p, app="android", base_url="http://pro.api.115.com", **request_kwargs
            ),
            cooldown=app_http_cooldown,
        ),
        api_name="share_snap_app_http",
        base_url="http://pro.api.115.com",
    )
    snap_app_https_info = ApiEndpointInfo(
        endpoint=ApiEndpointCooldown(
            api_callable=lambda p: client.share_snap_app(
                p, app="android", base_url="https://proapi.115.com", **request_kwargs
            ),
            cooldown=app_https_cooldown,
        ),
        api_name="share_snap_app_https",
        base_url="https://proapi.115.com",
    )
    snap_api_info = ApiEndpointInfo(
        endpoint=ApiEndpointCooldown(
            api_callable=lambda p: client.share_snap_cookie(p, **request_kwargs),
            cooldown=api_cooldown,
        ),
        api_name="share_snap",
        base_url=None,
    )
    repeating_pair = [snap_app_http_info, snap_app_https_info]
    first_page_api_pool = repeating_pair * 6
    first_page_api_pool.insert(6, snap_api_info)
    first_page_api_cycler = cycle(repeating_pair)

    def _job(
        api_info: ApiEndpointInfo,
        _cid: int,
        path_prefix: str,
        offset: int,
    ) -> Tuple[List[Dict[str, Any]], List[Tuple[int, str, int]]]:
        limit = 1_000
        if offset != 0:
            limit = 7_000
        payload = {
            "share_code": share_code,
            "receive_code": receive_code,
            "cid": _cid,
            "limit": limit,
            "offset": offset,
            "asc": asc,
            "o": order,
        }
        try:
            resp = api_info.endpoint(payload)
            check_response(resp)
        except Exception as e:
            api_info_str = f"API: {api_info.api_name}"
            if api_info.base_url:
                api_info_str += f", Base URL: {api_info.base_url}"
            api_info_str += f", Payload: {payload}"
            error_msg = f"{str(e)} | {api_info_str}"
            try:
                if e.args:
                    e.args = (error_msg,) + e.args[1:]
                else:
                    e.args = (error_msg,)
            except (TypeError, AttributeError):
                wrapper_msg = f"Exception occurred: {error_msg}"
                wrapper_e = RuntimeError(wrapper_msg)
                wrapper_e.__cause__ = e
                raise wrapper_e from e
            raise
        data = resp.get("data", {})
        count = data.get("count", 0)
        items = data.get("list", [])
        files_found = []
        subdirs_to_scan = []
        for attr in items:
            attr["share_code"] = share_code
            attr["receive_code"] = receive_code
            attr = normalize_attr(attr)
            name = posix_escape_name(attr["name"], repl="|")
            attr["name"] = name
            path = f"{path_prefix}/{name}" if path_prefix else f"/{name}"
            if attr["is_dir"]:
                subdirs_to_scan.append((int(attr["id"]), path, 0))
            else:
                attr["path"] = path
                files_found.append(attr)
        new_offset = offset + len(items)
        if new_offset < count and len(items) > 0:
            subdirs_to_scan.append((_cid, path_prefix, new_offset))
        return files_found, subdirs_to_scan

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        pending_futures: Set[Future] = set()
        initial_future = executor.submit(_job, next(first_page_api_cycler), cid, "", 0)
        pending_futures.add(initial_future)
        while pending_futures:
            for future in as_completed(pending_futures):
                pending_futures.remove(future)
                try:
                    files, subdirs = future.result()
                    for file_info in files:
                        yield file_info
                    for task_args in subdirs:
                        task_offset = task_args[2]
                        if task_offset > 0:
                            api_to_use = snap_api_info
                        else:
                            api_to_use = next(first_page_api_cycler)
                        new_future = executor.submit(_job, api_to_use, *task_args)
                        pending_futures.add(new_future)
                except Exception:
                    for f in pending_futures:
                        f.cancel()
                    executor.shutdown(wait=False, cancel_futures=True)
                    raise
                break


def get_pid_by_path(
    client: P115Client,
    path: str | PathLike | Path,
    mkdir: bool = True,
    update_cache: bool = True,
    by_cache: bool = True,
) -> int:
    """
    é€šè¿‡æ–‡ä»¶å¤¹è·¯å¾„è·å– ID

    :param client: 115 å®¢æˆ·ç«¯
    :param path: æ–‡ä»¶å¤¹è·¯å¾„
    :param mkdir: ä¸å­˜åœ¨åˆ™åˆ›å»ºæ–‡ä»¶å¤¹
    :param update_cache: æ›´æ–°æ–‡ä»¶è·¯å¾„ ID åˆ°ç¼“å­˜ä¸­
    :param by_cache: é€šè¿‡ç¼“å­˜è·å–

    :return int: æ–‡ä»¶å¤¹ IDï¼Œ0 ä¸ºæ ¹ç›®å½•ï¼Œ-1 ä¸ºè·å–å¤±è´¥
    """
    path = Path(path).as_posix()
    if path == "/":
        return 0
    if by_cache:
        pid = idpathcacher.get_id_by_dir(directory=path)
        if pid:
            return pid
    resp = client.fs_dir_getid(path)
    check_response(resp)
    pid = resp.get("id", -1)
    if pid == -1:
        return -1
    if pid == 0 and mkdir:
        resp = client.fs_makedirs_app(path, pid=0)
        check_response(resp)
        pid = resp["cid"]
        if update_cache:
            idpathcacher.add_cache(id=int(pid), directory=path)
        return pid
    if pid != 0:
        return pid
    return -1


def get_pickcode_by_path(
    client: P115Client,
    path: str | PathLike | Path,
) -> Optional[str]:
    """
    é€šè¿‡æ–‡ä»¶ï¼ˆå¤¹ï¼‰è·¯å¾„è·å– pick_code
    """
    db_helper = FileDbHelper()
    path = Path(path).as_posix()
    if path == "/":
        return None
    db_item = db_helper.get_by_path(path)
    if db_item:
        try:
            return db_item["pickcode"]
        except ValueError:
            return client.to_pickcode(db_item["id"])
    try:
        file_id = get_id(client=client, path=path)
        if file_id:
            return client.to_pickcode(file_id)
        return None
    except Exception:
        return None


class P115DiskHelper:
    """
    æ¨¡æ‹Ÿ P115Disk æ’ä»¶æ¥å£
    """

    def __init__(self, client: P115Client):
        if TYPE_CHECKING:
            from ...p115disk.p115_api import P115Api
        else:
            P115Api = Any

        try:
            from app.plugins.p115disk.p115_api import P115Api  # noqa: F401

            P115_API_AVAILABLE = True
        except (ImportError, Exception):
            P115_API_AVAILABLE = False

        if P115_API_AVAILABLE:
            self._p115_api = P115Api(client=client, disk_name="115ç½‘ç›˜Plus")

        self.oopserver_request = OOPServerRequest(max_retries=3, backoff_factor=1.0)

    def upload(
        self,
        target_dir: FileItem,
        local_path: Path,
        new_name: Optional[str] = None,
    ) -> Optional[FileItem]:
        """
        ä¸Šä¼ æ–‡ä»¶åˆ°äº‘ç›˜

        :param target_dir: ä¸Šä¼ ç›®æ ‡ç›®å½•é¡¹
        :param local_path: æœ¬åœ°æ–‡ä»¶è·¯å¾„
        :param new_name: ä¸Šä¼ åçš„æ–‡ä»¶åï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨æœ¬åœ°æ–‡ä»¶å

        :return: ä¸Šä¼ æˆåŠŸè¿”å›æ–‡ä»¶é¡¹ï¼Œå¤±è´¥è¿”å›None
        """

        def read_range_hash(range_str: str) -> str:
            start, end = map(int, range_str.split("-"))
            with open(local_path, "rb") as f:
                f.seek(start)
                chunk = f.read(end - start + 1)
                sha1 = hashes.Hash(hashes.SHA1())
                sha1.update(chunk)
                return sha1.finalize().hex().upper()

        def encode_callback(cb: str) -> str:
            return b64encode_as_string(cb)

        def send_upload_info(
            file_sha1: Optional[str],
            first_sha1: Optional[str],
            second_auth: bool,
            second_sha1: Optional[str],
            file_size: Optional[str],
            file_name: Optional[str],
            upload_time: Optional[int],
        ):
            """
            å‘é€ä¸Šä¼ ä¿¡æ¯
            """
            path = "/upload/info"
            headers = {"x-machine-id": configer.get_config("MACHINE_ID")}
            json_data = {
                "file_sha1": file_sha1,
                "first_sha1": first_sha1,
                "second_auth": second_auth,
                "second_sha1": second_sha1,
                "file_size": file_size,
                "file_name": file_name,
                "time": upload_time,
                "postime": datetime.now(timezone.utc)
                .isoformat(timespec="milliseconds")
                .replace("+00:00", "Z"),
            }
            try:
                response = self.oopserver_request.make_request(
                    path=path,
                    method="POST",
                    headers=headers,
                    json_data=json_data,
                    timeout=10.0,
                )

                if response is not None and response.status_code == 201:
                    logger.info(
                        f"ã€P115Diskã€‘ä¸Šä¼ ä¿¡æ¯æŠ¥å‘ŠæœåŠ¡å™¨æˆåŠŸ: {response.json()}"
                    )
                else:
                    logger.warn("ã€P115Diskã€‘ä¸Šä¼ ä¿¡æ¯æŠ¥å‘ŠæœåŠ¡å™¨å¤±è´¥ï¼Œç½‘ç»œé—®é¢˜")
            except Exception as e:
                logger.warn(f"ã€P115Diskã€‘ä¸Šä¼ ä¿¡æ¯æŠ¥å‘ŠæœåŠ¡å™¨å¤±è´¥: {e}")

        def send_upload_wait(target_name):
            """
            å‘é€ä¸Šä¼ ç­‰å¾…
            """
            if configer.notify and configer.upload_module_notify:
                post_message(
                    mtype=NotificationType.Plugin,
                    title="ã€115ç½‘ç›˜ã€‘ä¸Šä¼ æ¨¡å—å¢å¼º",
                    text=f"\nè§¦å‘ç§’ä¼ ç­‰å¾…ï¼š{target_name}\n",
                )

            try:
                self.oopserver_request.make_request(
                    path="/upload/wait",
                    method="POST",
                    headers={"x-machine-id": configer.get_config("MACHINE_ID")},
                    timeout=10.0,
                )
            except Exception:
                pass

        def send_upload_result_notify(
            success: bool,
            target_name: str,
            file_size: int,
            elapsed_time: Optional[float] = None,
            error_msg: Optional[str] = None,
        ):
            """
            å‘é€ä¸Šä¼ ç»“æœé€šçŸ¥

            :param success: æ˜¯å¦æˆåŠŸ
            :param target_name: æ–‡ä»¶å
            :param file_size: æ–‡ä»¶å¤§å°
            :param elapsed_time: è€—æ—¶ï¼ˆç§’ï¼‰
            :param error_msg: é”™è¯¯ä¿¡æ¯
            """
            if not configer.notify or not configer.upload_open_result_notify:
                return

            if success:
                size_str = StringUtils.str_filesize(file_size)
                time_str = f"{elapsed_time:.1f}ç§’" if elapsed_time else "æœªçŸ¥"
                post_message(
                    mtype=NotificationType.Plugin,
                    title="ã€115ç½‘ç›˜ã€‘ä¸Šä¼ æˆåŠŸ",
                    text=f"\næ–‡ä»¶åï¼š{target_name}\næ–‡ä»¶å¤§å°ï¼š{size_str}\nè€—æ—¶ï¼š{time_str}\n",
                )
            else:
                size_str = StringUtils.str_filesize(file_size)
                error_text = f"\næ–‡ä»¶åï¼š{target_name}\næ–‡ä»¶å¤§å°ï¼š{size_str}\n"
                if error_msg:
                    error_text += f"é”™è¯¯ä¿¡æ¯ï¼š{error_msg}\n"
                post_message(
                    mtype=NotificationType.Plugin,
                    title="ã€115ç½‘ç›˜ã€‘ä¸Šä¼ å¤±è´¥",
                    text=error_text,
                )

        if not local_path.exists():
            logger.error(f"ã€P115Diskã€‘æœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨: {local_path}")
            return None

        target_name = new_name or local_path.name
        target_path = Path(target_dir.path) / target_name

        # è·å–ç›®æ ‡ç›®å½•ID
        target_pid = target_dir.fileid

        # è®¡ç®—æ–‡ä»¶ç‰¹å¾å€¼
        file_size = local_path.stat().st_size
        file_sha1 = self._p115_api._calc_sha1(local_path)

        # æ¸…ç†ç¼“å­˜
        cache_id = self._p115_api._id_cache.get_id_by_dir(target_path.as_posix())
        if cache_id:
            self._p115_api._id_cache.remove(id=cache_id)
            self._p115_api._id_item_cache.remove(id=cache_id)

        # åˆå§‹åŒ–è¿›åº¦æ¡
        logger.info(f"ã€P115Diskã€‘å¼€å§‹ä¸Šä¼ : {local_path} -> {target_path}")
        progress_callback = transfer_process(local_path.as_posix())

        try:
            wait_start_time = perf_counter()
            send_wait = False
            while True:
                start_time = perf_counter()
                # Step 1: åˆå§‹åŒ–ä¸Šä¼ 
                init_resp = self._p115_api.client.upload_file_init(
                    filename=target_name,
                    filesize=file_size,
                    filesha1=file_sha1,
                    pid=target_pid,
                    read_range_bytes_or_hash=read_range_hash,
                )
                check_response(init_resp)

                logger.debug(f"ã€P115Diskã€‘ä¸Šä¼ åˆå§‹åŒ–ç»“æœ: {init_resp}")

                if not init_resp.get("state"):
                    logger.error(
                        f"ã€P115Diskã€‘åˆå§‹åŒ–ä¸Šä¼ å¤±è´¥: {init_resp.get('error')}"
                    )
                    return None

                # æ£€æŸ¥æ˜¯å¦ç§’ä¼ æˆåŠŸ
                if init_resp.get("reuse"):
                    logger.info(f"ã€P115Diskã€‘{target_name} ç§’ä¼ æˆåŠŸ")
                    progress_callback(100)
                    end_time = perf_counter()
                    elapsed_time = end_time - start_time
                    send_upload_info(
                        file_sha1,
                        None,
                        True,
                        None,
                        str(file_size),
                        target_name,
                        int(elapsed_time),
                    )
                    send_upload_result_notify(
                        success=True,
                        target_name=target_name,
                        file_size=file_size,
                        elapsed_time=elapsed_time,
                    )
                    return self._p115_api.get_item(target_path)

                # åˆ¤æ–­æ˜¯ç­‰å¾…ç§’ä¼ è¿˜æ˜¯ç›´æ¥ä¸Šä¼ 
                upload_module_skip_upload_wait_size = int(
                    configer.get_config("upload_module_skip_upload_wait_size") or 0
                )
                if (
                    upload_module_skip_upload_wait_size != 0
                    and file_size <= upload_module_skip_upload_wait_size
                ):
                    logger.info(
                        f"ã€P115Diskã€‘æ–‡ä»¶å¤§å° {file_size} å°äºæœ€ä½é˜ˆå€¼ï¼Œè·³è¿‡ç­‰å¾…æµç¨‹: {target_name}"
                    )
                    break

                if perf_counter() - wait_start_time > int(
                    configer.get_config("upload_module_wait_timeout")
                ):
                    logger.warn(
                        f"ã€P115Diskã€‘ç­‰å¾…ç§’ä¼ è¶…æ—¶ï¼Œè‡ªåŠ¨è¿›è¡Œä¸Šä¼ æµç¨‹: {target_name}"
                    )
                    break

                upload_module_force_upload_wait_size = int(
                    configer.get_config("upload_module_force_upload_wait_size") or 0
                )
                if (
                    upload_module_force_upload_wait_size != 0
                    and file_size >= upload_module_force_upload_wait_size
                ):
                    logger.info(
                        f"ã€P115Diskã€‘æ–‡ä»¶å¤§å° {file_size} å¤§äºæœ€é«˜é˜ˆå€¼ï¼Œå¼ºåˆ¶ç­‰å¾…æµç¨‹: {target_name}"
                    )
                    sleep(int(configer.get_config("upload_module_wait_time")))
                else:
                    try:
                        response = self.oopserver_request.make_request(
                            path="/speed/user_status/me",
                            method="GET",
                            headers={"x-machine-id": configer.get_config("MACHINE_ID")},
                            timeout=10.0,
                        )

                        if response is not None and response.status_code == 200:
                            resp = response.json()
                            if resp.get("status") != "slow":
                                logger.warn(
                                    f"ã€P115Diskã€‘ä¸Šä¼ é€Ÿåº¦çŠ¶æ€ {resp.get('status')}ï¼Œè·³è¿‡ç§’ä¼ ç­‰å¾…: {target_name}"
                                )
                                break

                            # è®¡ç®—ç­‰å¾…æ—¶é—´
                            default_wait_time = int(
                                configer.get_config("upload_module_wait_time")
                            )
                            sleep_time = default_wait_time
                            fastest_speed = resp.get("fastest_user_speed_mbps", None)
                            user_speed = resp.get("user_average_speed_mbps", None)
                            if fastest_speed and user_speed:
                                bs = user_speed * 0.2 + fastest_speed * 0.8
                                wt = file_size / (1024 * 1024) / bs
                                if wt > 10 * 60:
                                    wt = wt / (wt // (10 * 60) + 1)
                                if wt <= default_wait_time // 2:
                                    wt += default_wait_time // 2
                                sleep_time = int(wt)

                            logger.info(
                                f"ã€P115Diskã€‘ä¼‘çœ  {sleep_time} ç§’ï¼Œç­‰å¾…ç§’ä¼ : {target_name}"
                            )
                            if not send_wait:
                                send_upload_wait(target_name)
                                send_wait = True
                            sleep(sleep_time)
                        else:
                            logger.warn("ã€P115Diskã€‘è·å–ç”¨æˆ·ä¸Šä¼ é€Ÿåº¦é”™è¯¯ï¼Œç½‘ç»œé—®é¢˜")
                            break
                    except Exception as e:
                        logger.warn(f"ã€P115Diskã€‘è·å–ç”¨æˆ·ä¸Šä¼ é€Ÿåº¦é”™è¯¯: {e}")
                        break

            if configer.upload_module_skip_slow_upload:
                skip_upload_size = configer.get_config(
                    "upload_module_skip_slow_upload_size"
                )
                if skip_upload_size and skip_upload_size > 0:
                    if file_size >= skip_upload_size:
                        logger.warn(
                            f"ã€P115Diskã€‘{target_name} æ— æ³•ç§’ä¼ ï¼Œæ–‡ä»¶å¤§å° {file_size} å¤§äºç­‰äºé˜ˆå€¼ {skip_upload_size}ï¼Œè·³è¿‡ä¸Šä¼ "
                        )
                        send_upload_result_notify(
                            success=False,
                            target_name=target_name,
                            file_size=file_size,
                            error_msg=f"ç§’ä¼ å¤±è´¥ï¼Œæ–‡ä»¶å¤§å° {file_size} å¤§äºç­‰äºé˜ˆå€¼ {skip_upload_size}ï¼Œå·²è·³è¿‡ä¸Šä¼ ",
                        )
                        return None
                    else:
                        logger.info(
                            f"ã€P115Diskã€‘{target_name} æ— æ³•ç§’ä¼ ï¼Œä½†æ–‡ä»¶å¤§å° {file_size} å°äºé˜ˆå€¼ {skip_upload_size}ï¼Œç»§ç»­æ‰§è¡Œä¸Šä¼ "
                        )
                else:
                    logger.warn(f"ã€P115Diskã€‘{target_name} æ— æ³•ç§’ä¼ ï¼Œè·³è¿‡ä¸Šä¼ ")
                    send_upload_result_notify(
                        success=False,
                        target_name=target_name,
                        file_size=file_size,
                        error_msg="ç§’ä¼ å¤±è´¥ï¼Œå·²è·³è¿‡ä¸Šä¼ ",
                    )
                    return None

            # è·å–ä¸Šä¼ ä¿¡æ¯
            bucket_name = init_resp.get("bucket")
            object_name = init_resp.get("object")
            callback_info = init_resp.get("callback")

            if not all([bucket_name, object_name, callback_info]):
                logger.error(f"ã€P115Diskã€‘ä¸Šä¼ ä¿¡æ¯ä¸å®Œæ•´: {init_resp}")
                return None

            # Step 2: è·å–OSSä¸Šä¼ å‡­è¯
            (
                endpoint,
                access_key_id,
                access_key_secret,
                security_token,
                token_expiration,
            ) = self._p115_api._get_oss_token()
            logger.info(
                f"ã€P115Diskã€‘OSS Token è¿‡æœŸæ—¶é—´: {token_expiration.strftime('%Y-%m-%d %H:%M:%S UTC')}"
            )

            # Step 3: OSSåˆ†ç‰‡ä¸Šä¼ 
            auth = StsAuth(
                access_key_id=access_key_id,
                access_key_secret=access_key_secret,
                security_token=security_token,
            )
            bucket = Bucket(auth, endpoint, bucket_name)  # noqa
            part_size = determine_part_size(file_size, preferred_size=10 * 1024 * 1024)

            logger.info(
                f"ã€P115Diskã€‘å¼€å§‹åˆ†ç‰‡ä¸Šä¼ ï¼Œåˆ†ç‰‡å¤§å°: {part_size // 1024 // 1024}MB"
            )

            # åˆå§‹åŒ–åˆ†ç‰‡ä¸Šä¼ 
            upload_id = bucket.init_multipart_upload(
                object_name, params={"encoding-type": "url", "sequential": ""}
            ).upload_id
            parts = []

            # é€ä¸ªä¸Šä¼ åˆ†ç‰‡å¹¶æ›´æ–°è¿›åº¦
            with open(local_path, "rb") as fileobj:
                part_number = 1
                offset = 0
                while offset < file_size:
                    # æ£€æŸ¥æ˜¯å¦å–æ¶ˆä¸Šä¼ 
                    if global_vars.is_transfer_stopped(local_path.as_posix()):
                        logger.info(f"ã€P115Diskã€‘{local_path} ä¸Šä¼ å·²å–æ¶ˆï¼")
                        bucket.abort_multipart_upload(object_name, upload_id)
                        return None

                    # æ£€æŸ¥ token æ˜¯å¦å³å°†è¿‡æœŸï¼ˆæå‰ 5 åˆ†é’Ÿåˆ·æ–°ï¼‰
                    if self._p115_api._is_token_expiring(
                        token_expiration, threshold_minutes=5
                    ):
                        logger.info("ã€P115Diskã€‘Token å³å°†è¿‡æœŸï¼Œæ­£åœ¨åˆ·æ–°...")
                        try:
                            (
                                endpoint,
                                access_key_id,
                                access_key_secret,
                                security_token,
                                token_expiration,
                            ) = self._p115_api._get_oss_token()
                            # é‡æ–°åˆ›å»ºè®¤è¯å’Œ bucket å¯¹è±¡
                            auth = StsAuth(
                                access_key_id=access_key_id,
                                access_key_secret=access_key_secret,
                                security_token=security_token,
                            )
                            bucket = Bucket(auth, endpoint, bucket_name)  # noqa
                            logger.info(
                                f"ã€P115Diskã€‘Token åˆ·æ–°æˆåŠŸï¼Œæ–°çš„è¿‡æœŸæ—¶é—´: "
                                f"{token_expiration.strftime('%Y-%m-%d %H:%M:%S UTC')}"
                            )
                        except Exception as e:
                            logger.error(f"ã€P115Diskã€‘åˆ·æ–° Token å¤±è´¥: {str(e)}")
                            bucket.abort_multipart_upload(object_name, upload_id)
                            return None

                    num_to_upload = min(part_size, file_size - offset)

                    # ä¸Šä¼ åˆ†ç‰‡ï¼Œå¸¦é‡è¯•æœºåˆ¶å¤„ç† token è¿‡æœŸé”™è¯¯
                    max_retries = 2
                    for retry in range(max_retries):
                        try:
                            result = bucket.upload_part(
                                object_name,
                                upload_id,
                                part_number,
                                data=SizedFileAdapter(fileobj, num_to_upload),
                            )
                            parts.append(PartInfo(part_number, result.etag))
                            break  # ä¸Šä¼ æˆåŠŸï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                        except ServerError as e:
                            # æ£€æŸ¥æ˜¯å¦æ˜¯ token è¿‡æœŸé”™è¯¯
                            error_code = getattr(e, "code", "")
                            if (
                                error_code
                                in ("InvalidAccessKeyId", "SecurityTokenExpired")
                                and retry < max_retries - 1
                            ):
                                logger.warn(
                                    f"ã€P115Diskã€‘æ£€æµ‹åˆ° Token è¿‡æœŸé”™è¯¯ ({error_code})ï¼Œ"
                                    f"æ­£åœ¨åˆ·æ–°å¹¶é‡è¯•..."
                                )
                                # åˆ·æ–° token
                                (
                                    endpoint,
                                    access_key_id,
                                    access_key_secret,
                                    security_token,
                                    token_expiration,
                                ) = self._p115_api._get_oss_token()
                                auth = StsAuth(
                                    access_key_id=access_key_id,
                                    access_key_secret=access_key_secret,
                                    security_token=security_token,
                                )
                                bucket = Bucket(auth, endpoint, bucket_name)  # noqa
                                # éœ€è¦é‡æ–°å®šä½æ–‡ä»¶æŒ‡é’ˆ
                                fileobj.seek(offset)
                                continue
                            else:
                                # å…¶ä»–é”™è¯¯æˆ–é‡è¯•æ¬¡æ•°ç”¨å°½ï¼Œæ”¾å¼ƒä¸Šä¼ 
                                logger.error(f"ã€P115Diskã€‘ä¸Šä¼ åˆ†ç‰‡å¤±è´¥: {str(e)}")
                                bucket.abort_multipart_upload(object_name, upload_id)
                                raise

                    # æ›´æ–°åç§»å’Œåˆ†ç‰‡å·
                    offset += num_to_upload
                    part_number += 1

                    # å®æ—¶æ›´æ–°è¿›åº¦
                    progress = (offset * 100) / file_size
                    progress_callback(progress)
                    logger.debug(f"ã€P115Diskã€‘ä¸Šä¼ è¿›åº¦: {progress:.1f}%")

            # å®Œæˆä¸Šä¼ 
            progress_callback(100)

            # Step 4: å®ŒæˆOSSä¸Šä¼ å¹¶å›è°ƒ115æœåŠ¡å™¨
            headers = {
                "X-oss-callback": encode_callback(callback_info["callback"]),
                "x-oss-callback-var": encode_callback(callback_info["callback_var"]),
                "x-oss-forbid-overwrite": "false",
            }

            result = bucket.complete_multipart_upload(
                object_name, upload_id, parts, headers=headers
            )

            if result.status == 200:
                logger.info(f"ã€P115Diskã€‘{target_name} ä¸Šä¼ æˆåŠŸ")
                end_time = perf_counter()
                elapsed_time = end_time - start_time
                send_upload_result_notify(
                    success=True,
                    target_name=target_name,
                    file_size=file_size,
                    elapsed_time=elapsed_time,
                )
                end_time = perf_counter()
                elapsed_time = end_time - start_time
                send_upload_info(
                    file_sha1,
                    None,
                    False,
                    None,
                    str(file_size),
                    target_name,
                    int(elapsed_time),
                )
                return self._p115_api.get_item(target_path)
            else:
                logger.error(
                    f"ã€P115Diskã€‘{target_name} ä¸Šä¼ å¤±è´¥ï¼ŒçŠ¶æ€ç : {result.status}"
                )
                send_upload_result_notify(
                    success=False,
                    target_name=target_name,
                    file_size=file_size,
                    error_msg=f"é”™è¯¯ç : {result.status}",
                )
                return None

        except Exception as e:
            logger.error(f"ã€P115Diskã€‘ä¸Šä¼ å¤±è´¥: {local_path} - {str(e)}")
            send_upload_result_notify(
                success=False,
                target_name=target_name,
                file_size=file_size,
                error_msg=f"æœªçŸ¥é”™è¯¯: {str(e)}",
            )
            return None
